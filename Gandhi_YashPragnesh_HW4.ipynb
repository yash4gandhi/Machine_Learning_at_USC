{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Yash Pragnesh Gandhi <br><br> Github id: yash4gandhi <br><br> USC ID: 1150578261"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%pip install bootstrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bootstrapped.bootstrap as bs\n",
    "import bootstrapped.stats_functions as bs_stats\n",
    "import bootstrapped\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1 (a) Download the AReM data from: https://archive.ics.uci.edu/ml/datasets/\n",
    "Activity+Recognition+system+based+on+Multisensor+data+fusion+\\%28AReM\\\n",
    "%29 . The dataset contains 7 folders that represent seven types of activities. In\n",
    "each folder, there are multiple files each of which represents an instant of a human\n",
    "performing an activity.1 Each file containis 6 time series collected from activities\n",
    "of the same person, which are called avg rss12, var rss12, avg rss13, var rss13,\n",
    "vg rss23, and ar rss23. There are 88 instances in the dataset, each of which contains 6 time series and each time series has 480 consecutive values.<br><br>\n",
    "1 (b) Keep datasets 1 and 2 in folders bending1 and bending 2, as well as datasets 1,\n",
    "2, and 3 in other folders as test data and other datasets as train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = ['time','avg_rss12','var_rss12','avg_rss13','var_rss13','vg_rss23','ar_rss23']\n",
    "folder = ['bending1','bending2','cycling','lying','sitting','standing','walking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Arem/bending1/dataset1.csv\",names = col_name, header = None, sep = ',', skiprows = [i for i in range(0,5)])\n",
    "df['activity'] = 'bending1'\n",
    "instance = 1\n",
    "for i in folder:\n",
    "    for j in range(1,4):\n",
    "        if i == 'bending1':\n",
    "            if j == 2:\n",
    "                path = \"Arem/\"+i+\"/dataset\"+str(j)+\".csv\"\n",
    "                df1 = pd.read_csv(path,names = col_name, header = None, sep = ',', skiprows = [i for i in range(0,5)])\n",
    "                df1['activity'] = i\n",
    "                df = df.append(df1, ignore_index=True)\n",
    "                instance+=1\n",
    "                \n",
    "        elif i == 'bending2':\n",
    "            if j<3:\n",
    "                path = \"Arem/\"+i+\"/dataset\"+str(j)+\".csv\"\n",
    "                df1 = pd.read_csv(path,names = col_name, header = None, sep = ',', skiprows = [i for i in range(0,5)])\n",
    "                df1['activity'] = i\n",
    "                df = df.append(df1, ignore_index=True)\n",
    "                instance+=1\n",
    "        else:\n",
    "            path = \"Arem/\"+i+\"/dataset\"+str(j)+\".csv\"\n",
    "            df1 = pd.read_csv(path,names = col_name, header = None, sep = ',', skiprows = [i for i in range(0,5)])\n",
    "            df1['activity'] = i\n",
    "            df = df.append(df1, ignore_index=True)\n",
    "            instance+=1\n",
    "\n",
    "test = df\n",
    "print(\"Test data shape: \",test.shape)\n",
    "print(\"Number of instances in test data:\",instance)\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Arem/bending1/dataset3.csv\",names = col_name, header = None, sep = ',', skiprows = [i for i in range(0,5)])\n",
    "df['activity'] = 'bending1'\n",
    "instance = 1\n",
    "for i in folder:\n",
    "    for j in range(3,16):\n",
    "        if i == 'bending1':\n",
    "            if j < 8 and j>3:\n",
    "                path = \"Arem/\"+i+\"/dataset\"+str(j)+\".csv\"\n",
    "                df1 = pd.read_csv(path,names = col_name, header = None, sep = ',', skiprows = [i for i in range(0,5)])\n",
    "                df1['activity'] = i\n",
    "                df = df.append(df1, ignore_index=True)\n",
    "                instance+=1\n",
    "                \n",
    "        elif i == 'bending2':\n",
    "            if j < 7 and j!=4:\n",
    "                path = \"Arem/\"+i+\"/dataset\"+str(j)+\".csv\"\n",
    "                df1 = pd.read_csv(path,names = col_name, header = None, sep = ',', skiprows = [i for i in range(0,5)],usecols = range(7))\n",
    "                df1['activity'] = i\n",
    "                df = df.append(df1, ignore_index=True)\n",
    "                instance+=1\n",
    "                \n",
    "            if j == 4:\n",
    "                path = \"Arem/\"+i+\"/dataset\"+str(j)+\".csv\"\n",
    "                df1 = pd.read_csv(path,names = col_name, header = None, sep = ' ', skiprows = [i for i in range(0,5)],usecols = range(7))\n",
    "                df1['activity'] = i\n",
    "                df = df.append(df1, ignore_index=True)\n",
    "                instance+=1\n",
    "                \n",
    "        else:\n",
    "            if j > 3:\n",
    "                path = \"Arem/\"+i+\"/dataset\"+str(j)+\".csv\"\n",
    "                df1 = pd.read_csv(path,names = col_name, header = None, sep = ',', skiprows = [i for i in range(0,5)])\n",
    "                df1['activity'] = i\n",
    "                df = df.append(df1, ignore_index=True)\n",
    "                instance+=1\n",
    "                \n",
    "train = df\n",
    "print(\"Train data shape: \",train.shape)\n",
    "print(\"Number of instances in training data:\",instance)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1 (c) Feature Extraction\n",
    "Classification of time series usually needs extracting features from them. In this\n",
    "problem, we focus on time-domain features.<br><br>\n",
    "i. Research what types of time-domain features are usually used in time series\n",
    "classification and list them (examples are minimum, maximum, mean, etc).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Mean, Minimum, Quartiles 1 and 3, Standard deviations, Maximum and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ii. Extract the time-domain features minimum, maximum, mean, median, standard deviation, first quartile, and third quartile for all of the 6 time series\n",
    "in each instance. You are free to normalize/standardize features or use them\n",
    "directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = OrderedDict()\n",
    "x=0\n",
    "for i in folder:\n",
    "    \n",
    "    for j in range(1,16):\n",
    "        x+=1\n",
    "        if i == 'bending1':\n",
    "            if j < 8:\n",
    "                path = \"Arem/\"+i+\"/dataset\"+str(j)+\".csv\"\n",
    "                df1 = pd.read_csv(path,names = col_name, header = None, sep = ',', skiprows = [i for i in range(0,5)],usecols = range(7))\n",
    "                data_dict[i+\"/dataset\"+str(j)] = df1\n",
    "        elif i == 'bending2':\n",
    "            if j < 7 and j!=4:\n",
    "                path = \"Arem/\"+i+\"/dataset\"+str(j)+\".csv\"\n",
    "                df1 = pd.read_csv(path,names = col_name, header = None, sep = ',', skiprows = [i for i in range(0,5)],usecols = range(7))\n",
    "                data_dict[i+\"/dataset\"+str(j)] = df1\n",
    "            if j == 4:\n",
    "                path = \"Arem/\"+i+\"/dataset\"+str(j)+\".csv\"\n",
    "                df1 = pd.read_csv(path,names = col_name, header = None, sep = ' ', skiprows = [i for i in range(0,5)],usecols = range(7))\n",
    "                data_dict[i+\"/dataset\"+str(j)] = df1\n",
    "        else:\n",
    "                path = \"Arem/\"+i+\"/dataset\"+str(j)+\".csv\"\n",
    "                df1 = pd.read_csv(path,names = col_name, header = None, sep = ',', skiprows = [i for i in range(0,5)],usecols = range(7))\n",
    "                data_dict[i+\"/dataset\"+str(j)] = df1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Instances loaded: \",len(data_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Features to be extracted:  9\n"
     ]
    }
   ],
   "source": [
    "column_name = ['avg_rss12','var_rss12','avg_rss13','var_rss13','vg_rss23','ar_rss23']\n",
    "features = ['min','max','mean','median','std','1stQuart','2ndQuart']\n",
    "x = 1\n",
    "result = []\n",
    "l = ['Instance']\n",
    "for i in column_name:\n",
    "    for j in features:\n",
    "        l.append(i+\"_\"+j+str(x))\n",
    "    x+=1\n",
    "result.append(l)\n",
    "print(\"Total Features to be extracted: \",len(result[0])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "for i in data_dict:\n",
    "    x+=1\n",
    "    l = [x]\n",
    "    for j in column_name:\n",
    "        l.extend([data_dict[i][j].min(),data_dict[i][j].max(),round(data_dict[i][j].mean(),2),round(data_dict[i][j].median(),2),\n",
    "        round(data_dict[i][j].std(),2), float(data_dict[i][j].quantile([.25]).iloc[0]), float(data_dict[i][j].quantile([.75]).iloc[0])])\n",
    "    result.append(l)\n",
    "# resultdf = pd.DataFrame(result[1:], columns = result[0])\n",
    "# resultdf    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultdf = pd.DataFrame(result[1:], columns = result[0])\n",
    "resultdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>iii. Estimate the standard deviation of each of the time-domain features you\n",
    "extracted from the data. Then, use Python’s bootstrapped or any other\n",
    "method to build a 90% bootsrap confidence interval for the standard deviation\n",
    "of each feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newresult = resultdf.drop(columns = ['Instance'])\n",
    "feature_std = newresult.std().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_list = [['Feature','Standard Deviation','Confidence Interval lower bound','Confidence Interval upper bound']]\n",
    "\n",
    "for i in range(len(newresult.columns)):\n",
    "    feature_bs = bs.bootstrap(np.array(newresult[newresult.columns[i]]), bs_stats.std, alpha = 0.10)\n",
    "    l = [newresult.columns[i],feature_std[i],feature_bs.lower_bound,feature_bs.upper_bound]\n",
    "    bootstrap_list.append(l)\n",
    "\n",
    "bootstrapdf = pd.DataFrame(bootstrap_list[1:], columns = bootstrap_list[0])\n",
    "bootstrapdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>iv. Use your judgement to select the three most important time-domain features\n",
    "(one option may be min, mean, and max)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: As per the above data the most important time-domain features are Minimum, Maximum and Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. ISLR 3.7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> I collect a set of data (n = 100 observations) containing a single predictor and a quantitative\n",
    "response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e.\n",
    "Y = β0 + β1X + β2X2 + β3X3 + epsilon.<br>\n",
    "(a) Suppose that the true relationship between X and Y is linear, i.e. Y = β0+β1X +epsilon. Consider\n",
    "the training residual sum of squares (RSS) for the linear regression, and also the training RSS\n",
    "for the cubic regression. Would we expect one to be lower than the other, would we expect\n",
    "them to be the same, or is there not enough information to tell? Justify your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer (a):<br>\n",
    "The Cubic regression model might have lower RSS than the linear regression. Because owing to the extra terms in cubic regression, the model would be more flexible and can reduce the RSS.The cubic regression might overfit on training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>(b) Answer (a) using test rather than training RSS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer (b):<br>\n",
    "          The Linear Regression model will have lower RSS as compared to the Cubic one. This is because as Cubic regression was more flexible and might have over fitted the training data, the test data would lead to increase in error for cubic model. The Linear model having true relationship will perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>(c) Suppose that the true relationship between X and Y is not linear,\n",
    "but we don’t know how far it is from linear. Consider the training\n",
    "RSS for the linear regression, and also the training RSS for the\n",
    "cubic regression. Would we expect one to be lower than the\n",
    "other, would we expect them to be the same, or is there not\n",
    "enough information to tell? Justify your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer (c):<br>\n",
    "The Cubic Regression Model will have lower traning RSS as compared to the linear model. The cubic model have more terms and  hence it has more flexibility to fit the model more accurately than linear. This in turn would reduce the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>(d) Answer (c) using test rather than training RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer (d):<br>\n",
    "The question does not provide adequate information to suggest which model will have lower Test RSS. This because the test error depends on the kind of relationship between X and Y. If it is more near to cubic then cubic will have lower RSS and the opposite will happen if it is more close to linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Time Series Classification Part 2: Binary and Multiclass Classification<br><br>\n",
    "(a) Binary Classification Using Logistic Regression3<br>\n",
    "i. Assume that you want to use the training set to classify bending from other\n",
    "activities, i.e. you have a binary classification problem. Depict scatter plots\n",
    "of the features you specified in 1(c)iv extracted from time series 1, 2, and 6 of\n",
    "each instance, and use color to distinguish bending vs. other activities. (See\n",
    "p. 129 of the textbook).4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = OrderedDict()\n",
    "x=0\n",
    "for i in folder:\n",
    "    \n",
    "    for j in range(3,16):\n",
    "        x+=1\n",
    "        if i == 'bending1':\n",
    "            if j < 8:\n",
    "                path = \"Arem/\"+i+\"/dataset\"+str(j)+\".csv\"\n",
    "                df1 = pd.read_csv(path,names = col_name, header = None, sep = ',', skiprows = [i for i in range(0,5)],usecols = range(7))\n",
    "                train_data_dict[i+\"/dataset\"+str(j)] = df1\n",
    "        elif i == 'bending2':\n",
    "            if j < 7 and j!=4:\n",
    "                path = \"Arem/\"+i+\"/dataset\"+str(j)+\".csv\"\n",
    "                df1 = pd.read_csv(path,names = col_name, header = None, sep = ',', skiprows = [i for i in range(0,5)],usecols = range(7))\n",
    "                train_data_dict[i+\"/dataset\"+str(j)] = df1\n",
    "            if j == 4:\n",
    "                path = \"Arem/\"+i+\"/dataset\"+str(j)+\".csv\"\n",
    "                df1 = pd.read_csv(path,names = col_name, header = None, sep = ' ', skiprows = [i for i in range(0,5)],usecols = range(7))\n",
    "                train_data_dict[i+\"/dataset\"+str(j)] = df1\n",
    "        elif j>3:\n",
    "                path = \"Arem/\"+i+\"/dataset\"+str(j)+\".csv\"\n",
    "                df1 = pd.read_csv(path,names = col_name, header = None, sep = ',', skiprows = [i for i in range(0,5)],usecols = range(7))\n",
    "                train_data_dict[i+\"/dataset\"+str(j)] = df1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Instances loaded:  69\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Instances loaded: \",len(train_data_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Features to be extracted:  9\n"
     ]
    }
   ],
   "source": [
    "column_name = ['avg_rss12','var_rss12','ar_rss23']\n",
    "features = ['mean','median','std']\n",
    "x = 1\n",
    "result = []\n",
    "l = ['Instance']\n",
    "for i in column_name:\n",
    "    for j in features:\n",
    "        l.append(i+\"_\"+j+str(x))\n",
    "    x+=1\n",
    "l.append('Activity')\n",
    "result.append(l)\n",
    "print(\"Total Features to be extracted: \",len(result[0])-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=0\n",
    "for i in train_data_dict:\n",
    "    x+=1\n",
    "    l = [x]\n",
    "    for j in column_name:     \n",
    "        l.extend([round(train_data_dict[i][j].mean(),2),round(train_data_dict[i][j].median(),2), round(train_data_dict[i][j].std(),2)])\n",
    "    temp = ''\n",
    "    if 'bending' in i:\n",
    "        temp = 'bending'\n",
    "    else:\n",
    "        temp = 'Other'\n",
    "    l.extend([temp])\n",
    "    result.append(l)\n",
    "resultdf = pd.DataFrame(result[1:], columns = result[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>avg_rss12_mean1</th>\n",
       "      <th>avg_rss12_median1</th>\n",
       "      <th>avg_rss12_std1</th>\n",
       "      <th>var_rss12_mean2</th>\n",
       "      <th>var_rss12_median2</th>\n",
       "      <th>var_rss12_std2</th>\n",
       "      <th>ar_rss23_mean3</th>\n",
       "      <th>ar_rss23_median3</th>\n",
       "      <th>ar_rss23_std3</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>33.59</td>\n",
       "      <td>34.25</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.26</td>\n",
       "      <td>2.46</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.11</td>\n",
       "      <td>1.64</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>34.32</td>\n",
       "      <td>35.25</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.46</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.73</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>34.55</td>\n",
       "      <td>35.25</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.37</td>\n",
       "      <td>4.13</td>\n",
       "      <td>2.36</td>\n",
       "      <td>3.34</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>34.87</td>\n",
       "      <td>35.25</td>\n",
       "      <td>4.53</td>\n",
       "      <td>4.38</td>\n",
       "      <td>3.92</td>\n",
       "      <td>2.44</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.27</td>\n",
       "      <td>1.69</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>34.47</td>\n",
       "      <td>35.00</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.36</td>\n",
       "      <td>3.96</td>\n",
       "      <td>2.39</td>\n",
       "      <td>3.34</td>\n",
       "      <td>3.09</td>\n",
       "      <td>1.70</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Instance  avg_rss12_mean1  avg_rss12_median1  avg_rss12_std1  \\\n",
       "64        65            33.59              34.25            4.65   \n",
       "65        66            34.32              35.25            4.75   \n",
       "66        67            34.55              35.25            4.84   \n",
       "67        68            34.87              35.25            4.53   \n",
       "68        69            34.47              35.00            4.80   \n",
       "\n",
       "    var_rss12_mean2  var_rss12_median2  var_rss12_std2  ar_rss23_mean3  \\\n",
       "64             4.58               4.26            2.46            3.26   \n",
       "65             4.46               3.90            2.60            3.43   \n",
       "66             4.37               4.13            2.36            3.34   \n",
       "67             4.38               3.92            2.44            3.42   \n",
       "68             4.36               3.96            2.39            3.34   \n",
       "\n",
       "    ar_rss23_median3  ar_rss23_std3 Activity  \n",
       "64              3.11           1.64    Other  \n",
       "65              3.20           1.73    Other  \n",
       "66              3.08           1.66    Other  \n",
       "67              3.27           1.69    Other  \n",
       "68              3.09           1.70    Other  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultdf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = resultdf.columns.tolist()\n",
    "l.remove('Instance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['avg_rss12_mean1',\n",
       " 'avg_rss12_median1',\n",
       " 'avg_rss12_std1',\n",
       " 'var_rss12_mean2',\n",
       " 'var_rss12_median2',\n",
       " 'var_rss12_std2',\n",
       " 'ar_rss23_mean3',\n",
       " 'ar_rss23_median3',\n",
       " 'ar_rss23_std3',\n",
       " 'Activity']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-700928b08d7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainDataDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Activity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'bending'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'red'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'non-bending'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'blue'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "sns.pairplot(trainDataDF[l], hue='Activity', palette={'bending':'red','non-bending':'blue'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
